<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>My Super Juicy Blog</title><link>http://10.179.5.96:33000/</link><description>Recent content on My Super Juicy Blog</description><generator>Hugo -- 0.142.0</generator><language>en</language><lastBuildDate>Sat, 01 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://10.179.5.96:33000/index.xml" rel="self" type="application/rss+xml"/><item><title>What is GitOps</title><link>http://10.179.5.96:33000/posts/what-is-gitops/</link><pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate><guid>http://10.179.5.96:33000/posts/what-is-gitops/</guid><description>&lt;h2 id="what-is">What is?&lt;/h2>
&lt;p>A way to mange infrastructure by using Git as the single control centre. Instead of manually setting up or updating the system, config file that describe the system is stored on Git. When these files are change, automation tools (Argo CD or Flux) will automatically update the systems to match the new config.&lt;/p>
&lt;p>It makes the deployment consistent, easy to track (version control), and quick to roll back if something goes wrong.&lt;/p></description></item><item><title>Blog Development</title><link>http://10.179.5.96:33000/posts/blog-development/</link><pubDate>Wed, 29 Jan 2025 00:00:00 +0000</pubDate><guid>http://10.179.5.96:33000/posts/blog-development/</guid><description>&lt;h2 id="road-map">Road Map&lt;/h2>
&lt;p>![[Pasted image 20250129191000.png]]&lt;/p>
&lt;h2 id="step-1-obsidian">Step 1: Obsidian&lt;/h2>
&lt;ul>
&lt;li>Start writing blog in obsidian under a &lt;code>post&lt;/code> folder&lt;/li>
&lt;/ul>
&lt;h2 id="step-2-hugo">Step 2: Hugo&lt;/h2>
&lt;ul>
&lt;li>Use Hugo to turn the markdown in to website code.&lt;/li>
&lt;/ul>
&lt;h3 id="install-go">Install Go&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>Download Go for Linux&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>wget https://go.dev/dl/go1.23.5.linux-arm64.tar.gz
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>&lt;strong>Remove any previous Go installation&lt;/strong> by deleting the &lt;code>/usr/local/go&lt;/code> folder (if it exists), then extract the archive you just downloaded into &lt;code>/usr/local&lt;/code>, creating a fresh Go tree in &lt;code>/usr/local/go&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>rm -rf /usr/local/go &lt;span style="color:#f92672">&amp;amp;&amp;amp;&lt;/span> tar -C /usr/local -xzf go1.23.5.linux-amd64.tar.gz
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Add &lt;code>/usr/local/go/bin&lt;/code> to the &lt;code>PATH&lt;/code> environment variable.&lt;/p></description></item><item><title>TestBlog</title><link>http://10.179.5.96:33000/posts/testblog/</link><pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate><guid>http://10.179.5.96:33000/posts/testblog/</guid><description>&lt;h1 id="hello-world-this-is-my-first-blog-post">Hello world! This is my first blog post!!!&lt;/h1></description></item><item><title/><link>http://10.179.5.96:33000/posts/k-nearest-neigbours-k-nn/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://10.179.5.96:33000/posts/k-nearest-neigbours-k-nn/</guid><description>&lt;p>#extra&lt;/p>
&lt;p>&lt;strong>k-NN&lt;/strong> is a simple machine learning algorithm used for classification or regression. It works by comparing a new data point to its closest &amp;ldquo;neighbors&amp;rdquo; in the dataset.&lt;/p>
&lt;hr>
&lt;h3 id="how-it-works">How It Works:&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>Input&lt;/strong>: A new data point that you want to classify or predict.&lt;/li>
&lt;li>&lt;strong>Find Neighbors&lt;/strong>: Look for the &lt;strong>k nearest points&lt;/strong> (neighbors) in the training data based on a distance metric (e.g., Euclidean distance).&lt;/li>
&lt;li>&lt;strong>Decision&lt;/strong>:
&lt;ul>
&lt;li>&lt;strong>For Classification&lt;/strong>:
&lt;ul>
&lt;li>Assign the most common class among the neighbors (majority vote).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>For Regression&lt;/strong>:
&lt;ul>
&lt;li>Take the average value of the neighbors.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;hr>
&lt;h3 id="key-concepts">Key Concepts:&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>k&lt;/strong>: The number of neighbors to consider.
&lt;ul>
&lt;li>Small &lt;strong>k&lt;/strong> (e.g., 1): Sensitive to noise.&lt;/li>
&lt;li>Large &lt;strong>k&lt;/strong>: Smoother but may ignore details.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Distance Metric&lt;/strong>:
&lt;ul>
&lt;li>Common: Euclidean distance.&lt;/li>
&lt;li>Others: Manhattan, cosine similarity, etc.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="pros">Pros:&lt;/h3>
&lt;ul>
&lt;li>Simple to understand and implement.&lt;/li>
&lt;li>No training phase (lazy learning).&lt;/li>
&lt;/ul>
&lt;h3 id="cons">Cons:&lt;/h3>
&lt;ul>
&lt;li>Can be slow for large datasets (requires searching neighbors for every prediction).&lt;/li>
&lt;li>Sensitive to irrelevant features or different scales (requires normalization).&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="example">Example:&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>You want to classify a fruit&lt;/strong>:
&lt;ul>
&lt;li>Measure the size and color of the fruit.&lt;/li>
&lt;li>Compare it to its closest k fruits in the dataset.&lt;/li>
&lt;li>Assign it the class (e.g., apple, orange) most common among the k neighbors.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item></channel></rss>